\documentclass[notheorems]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{polski}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{algorithm,algorithmic}
\usepackage{natbib} %bibtex
\usepackage[polish]{dyschemist}
\newtheorem{tw}{Twierdzenie}
%\usepackage{dyschemist}
\author{Anna Szczepaniak, 210094}
\title{Algorytmy dekompozycji QR}
\date{19.09.2019}
\institute{Praca licencjacka przygotowana pod opieką dr inż. Piotra Kowalskiego}
\newcommand{\parauporzadkowana}[2]{\left\langle {#1}; {#2} \right\rangle}
\newcommand{\zbior}[1]{\left\lbrace {#1} \right\rbrace }
\newcommand{\domkniecie}[1]{\left[ {#1} \right] }
\newcommand{\notiff}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{"}$\hidewidth\cr$\iff$}}}}
%\newcommand{\tuple}[1]{\left\langle {#1} \right\rangle}
\newcommand{\vr}[1]{\mathbf{#1}}
\newcommand{\mx}[1]{{#1}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\proj}[2]{\frac{\scalar{#2}{#1}}{\scalar{#1}{#1}}}

\usepackage{Sweave}
\begin{document}
\begin{frame}
\titlepage
\end{frame}


\begin{frame}
\begin{theorem}[Twierdzenie(Grama-Schmidta)\citep{poreda11}] \label{theorem-gram-schmidt}
Dla każdego układu liniowo niezależnego wektorów $(\vr{x}_{1},\ldots,\vr{x}_{n})$ w przestrzeni euklidesowej istnieje układ ortonormalny $(\vr{b}_{1},\ldots, \vr{b}_{n})$ taki, że 
$$
span(\vr{b}_{1},\ldots, \vr{b}_{k}) = span(\vr{x}_{1},\ldots, \vr{x}_{k})
$$
dla każdej liczby $k$ ze zbioru $(1,\ldots,n)$.
\end{theorem}
\end{frame}


\begin{frame}
\begin{lemma}[O postaci macierzowej w algorytmie Grama-Schmidta] \label{lemma-matrix-formulation-of-gs}
Powyżej omówiony algorytm Grama Schmidta jest równoważny następującym transformacjom macierzy. Załóżmy, że wektory $v_{1}, \ldots, v_n$ są kolumnami macierzy $\mx{A}$ oraz że są liniowo niezależne. Wtedy z twierdzenia \ref{theorem-gram-schmidt} istnieją wektory $u_1, \ldots, u_n$ , które są transformacją wektorów $v_1, \ldots, v_n$ przez algorytm Grama-Schmidta. Niech $\mx{U}$ będzie macierzą utworzoną kolumnowo z~tych wektorów $u_1, \ldots, u_n$. Wtedy
{\fontsize{4}{4}
$$
U = A \cdot 
\begin{bmatrix}
1 & -\proj{u_1}{v_2} & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix} \cdot
\begin{bmatrix}
1 & 0 & -\proj{u_1}{v_3} &\cdots & 0 \\
0 & 1 & -\proj{u_2}{v_3} &\cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots &  & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
 \cdots
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 & -\proj{u_1}{v_n} \\
0 & 1 & 0 & \cdots & 0 &-\proj{u_2}{v_n} \\
0 & 0 & 1 & \cdots & 0 & -\proj{u_3}{v_n} \\
\vdots & \vdots & \vdots &  & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & -\proj{u_{n-1}}{v_n} \\
0 & 0 & 0 & \cdots & 0 & 1
\end{bmatrix}
$$ 
}
\end{lemma}
\end{frame}

\begin{frame}
\begin{lemma}[Macierz transformująca algorytmu Grama-Schmidta]\label{lemma-gram-schmidt-matrix}
Niech $\mx{A} \in \setR^{m \times n}$ będzie macierzą, której kolumny są liniowo niezależne. Niech $\mx{U} \in \setR^{m \times n}$ będzie macierzą uzyskaną poprzez połączenie jako kolumn wektorów uzyskanych z algorytmu Grama-Schmidta. Wtedy istnieje trójkąta górna macierz $\mx{T} \in \setR^{n \times n}$ taka, że
$$
\mx{U} = \mx{A} \cdot \mx{T},
$$
gdzie $\mx{T} = [t_{ij}]_{i = 1, \ldots , n}^{j = 1, \ldots, n}$ i $t_{kk} = 1$, dla dowolnego $k \in \set{1,\ldots,n}$.
\end{lemma}
\end{frame}



\begin{frame}
\begin{theorem}[O rozkładzie QR]\label{theorem-qr-decomposition}
Niech $\mx{A} \in \setR^{m \times n}$, gdzie $m\ge n$, której kolumny są liniowo niezależne. Istnieje wtedy jedyny rozkład $\mx{QR}$, tzn. że istnieją takie macierze $\mx{Q}$ i $\mx{R}$, że
$$
\mx{A} = \mx{Q} \mx{R}
$$ 
i
\begin{itemize}
\item macierz $\mx{Q} \in \setR^{m \times n} $ jest taka, że 
$$
Q^{T}\cdot Q=D,
$$
gdzie $D= diag (d_{1}, d_{2}, ..., d_{n})$, oraz $d_{k}>0$ dla $k = 1, 2, \ldots, n$, oraz
\item macierz $\mx{R} \in \setR^{n \times n}$ jest trójkątną górną spełniającą dodatkowo warunek 
$$
r_{kk}= 1 
$$ 
dla wszystkich $k = 1, 2, \ldots, n$.
\end{itemize} 
\end{theorem}
\end{frame}

\begin{frame}
\begin{remark}
Rozkładem $\mx{QR}$ nazwiemy również sytuacje, gdy 
$$
\mx{A}=\mx{Q}\mx{R}
$$
oraz $\transpose{\mx{Q}}\mx{Q}=\mx{I}$ i $\mx{R}$ jest macierzą trójkątną górną, niekoniecznie z $1$ na przekątnej. Powodem tego jest, że prezentowana tu postać oraz postać z twierdzenia \ref{theorem-qr-decomposition} są sobie równoważne.  
\end{remark}
\end{frame}


\begin{frame}
\begin{definition}[Macierz Householdera]
Macierzą Householdera $H$, zwaną również refleksją, nazywamy macierz postaci 
$$
\mx{H}=\mx{I}-2\cdot \vr{v}\cdot \vr{v}^{T},
$$
gdzie $\Vert v \Vert_{2} = 1$.
\end{definition}
\end{frame}

\begin{frame}
\begin{theorem} [Transformacja Householdera]
Niech  $\vr{v}\in R^{m}, $ i $\vr{v}\neq 0. $ Wówczas transformacją Householdera nazywamy macierz postaci:
$$
\mx{H}=\mx{I}-W\vr{v}\vr{v}^{T},
$$
gdzie
$$
W={\frac {2}{\vr{v}^{T}\vr{v}}}.
$$ 
Macierz H jest macierzą symetryczną i ortogonalną.
\end{theorem}
\end{frame}

\begin{frame}
\begin{example}
\begin{enumerate}
\item Wyznaczamy $P_{1}$ tak, że:
$$
\mx{A_{1} \equiv {P_{1}\cdot A}}= \begin{bmatrix}
x & x & x & x \\
0 & x & x & x  \\
0 & x & x & x \\
0 & x & x & x  \\
0 & x & x & x 
\end{bmatrix}.
$$
\item Wyznaczamy 
$$
P_{2} = \left[ 
\begin{array}{c|c}
1 & 0 \\\hline
0 & P_2'
\end{array}
\right]
$$ 
tak, że
$$
\mx{A_{2} \equiv {P_{2}\cdot A_{1}}}= \begin{bmatrix}
x & x & x & x \\
0 & x & x & x  \\
0 & 0 & x & x \\
0 & 0 & x & x  \\
0 & 0 & x & x 
\end{bmatrix}.
$$
\end{enumerate}
\end{example}
\end{frame}



\begin{frame}
\begin{definition}[Macierz Givensa]
Niech $ i,j \in {1, \ldots, n}$ i $\theta \in \setR$. Macierz $\mx{R}(i,j,\theta) \in \setR^{n\times n}$ zdefiniowana następująco:
$$
\mx{R}(i,j,\theta) = \begin{bmatrix}
1       & 0     & \cdots &    0       & \cdots &    0       & \cdots & 0 & 0& \\
0       & 1     & \cdots &     0      & \cdots &     0      &  \cdots& 0 & 0 &   \\
0       & 0     & \ddots &     0      &  \cdots&     0      &  \cdots& 0 & 0 &  \\
\vdots  &\vdots &  \cdots& \cos\theta & \cdots & -\sin\theta&  \cdots& 0 & 0& \\
0       &  0    & \cdots & \vdots     &  \ddots&  \vdots    & \cdots & 0 & 0& \\
\vdots  & \vdots& \cdots & \sin\theta & \cdots & \cos\theta &  \cdots& 0 & 0& \\
0       & 0     & \cdots &    0       &  \cdots&  0         & \ddots &  0& 0& \\
\vdots  & \vdots&\cdots  &  \cdots    &  \cdots&  \cdots    & \cdots & 1 & 0&\\
0       & 0     & \cdots &     0      &  \cdots&    0       &  \cdots& 0 & 1 & \\       
\end{bmatrix}
$$
nazywamy macierzą rotacji Givensa.
\end{definition}
\end{frame}

\begin{frame}
\begin{example}
{\scriptsize
$$
\begin{bmatrix}
x & x & x & x \\
x & x & x & x \\
x & x & x & x \\
x & x & x & x   
\end{bmatrix} \underrightarrow{\mx{G}(1,2)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
x & x & x & x \\
x & x & x & x   
\end{bmatrix} \underrightarrow{\mx{G}(1,3)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & x & x & x \\
x & x & x & x   
\end{bmatrix} \underrightarrow{\mx{G}(1,4)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & x & x & x \\
0 & x & x & x   
\end{bmatrix}  
$$
}
$$
\begin{bmatrix}
x & x & x & x \\
0 & x & x & x  \\
0 & x & x & x \\
0 & x & x & x  \\
0 & x & x & x 
\end{bmatrix}  \underrightarrow{\mx{G}(2,3)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & x & x & x   
\end{bmatrix} \underrightarrow{\mx{G}(2,4)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & x & x   
\end{bmatrix} 
$$   

$$
\begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & x & x   
\end{bmatrix} \underrightarrow{\mx{G}(3,4)} \begin{bmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & 0 & x   
\end{bmatrix} 
$$
\end{example}
\end{frame}


\begin{frame}[fragile]
\begin{Schunk}
\begin{Sinput}
> house = function(x) {
+   norm_x = sqrt(t(x)\cdot x)
+   if (x[1] > 0) {
+     x[1] = x[1] + norm_x;
+   } else {
+     x[1] = x[1] - norm_x;
+   }
+   return(x)
+ }
\end{Sinput}
\end{Schunk}
\end{frame}


\begin{frame}[fragile]
\begin{Schunk}
\begin{Sinput}
> qr_householer = function(A)
+ {
+  R = A
+  dimm = dim(A)
+  n = dimm[2]
+   m = dimm[1]
+   Q = diag(n)
+   for (i in 1:min(m-1,n)){
+     u = house(R[i:m,i])
+     den =  (t(u) %*% u)[1,1]
+     P = diag(n+1-i) - 2/den * ( u%*% t(u)  ) 
+     R[i:m,i:n] = P %*% R[i:m,i:n]
+     if (i>1)
+       Q[i:n,1:(i-1)] = P %*% Q[i:n,1:(i-1)]
+     Q[i:n,i:n] = P %*% Q[i:n,i:n]
+   }
+   return(list(R=R, Q=t(Q)))
+ }
\end{Sinput}
\end{Schunk}
\end{frame}



\begin{frame}[fragile]
\begin{Schunk}
\begin{Sinput}
> givens = function(i,j,A){
+   dimm = dim(A)
+   n = dimm[1]
+   G = diag(n)
+   p = sqrt((A[i,i])^2 + (A[j,i])^2)
+   G[i,i] = A[i,i]/ p
+   G[i,j] = (A[j,i])/p
+   G[j,i] = -(A[j,i])/p
+   G[j,j] = A[i,i]/ p
+   
+ return(G)  
+   
+ }
\end{Sinput}
\end{Schunk}
\end{frame}

\begin{frame}[fragile]
\begin{Schunk}
\begin{Sinput}
> qr_givens = function(A){
+   R = A
+   
+   n = dim(A)[2]
+   m = dim(A)[1]
+   Q = diag(n)
+   for (i in 1:(n-1)) {
+     for (j in (i+1):m) {
+       G = givens(i,j,R)
+       R = G %*% R
+       Q = Q %*% t(G)
+     }
+   }
+   return(list(R=R, Q=Q))
+ }
\end{Sinput}
\end{Schunk}
\end{frame}


\begin{frame}[fragile]
{\scriptsize
Z eksperymentów, które przeprowadziliśmy wypływa wniosek, iż nasze algorytmy dekompozycji QR są nieco gorsze jakościowo od gotowej funkcji w R. 
}
\includegraphics[width=0.7\textwidth]{licencjat-027}
\end{frame}

\begin{frame}
\frametitle{Bibliografia}
\begin{thebibliography}{1}

\bibitem{dokumentacjaR}
Dokumentacja online języka r.
\newblock \url{https://www.rdocumentation.org}.


\bibitem{bjorck14}
Åke Björck.
\newblock {\em Numerical Methods for Least Squares Problems}.
\newblock Siam, 1996.

\bibitem{demmel12}
James~W. Demmel.
\newblock {\em Applied Numerical Linear Algebra}.
\newblock Siam, 1997.

\bibitem{poreda11}
Jacek Jędrzejewski and Tadeusz Poreda.
\newblock {\em Algebra liniowa z elementami geometrii analitycznej}.
\newblock Politechnika Łodzka, 2011.

\bibitem{wilkinson13}
James~H. Wilkinson.
\newblock {\em The algebraic eigenvalue problem}.
\newblock Clarendon Press, Oxford University Press, 1988.

\end{thebibliography}
\end{frame}



\bibliographystyle{plain}
\bibliography{bibliografia}



\end{document}