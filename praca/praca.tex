\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{polski}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[polish, chapter]{dyschemist}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}


\newcommand{\vr}[1]{\mathbf{#1}}
\newcommand{\mx}[1]{{#1}}

\newcommand{\proj}[2]{\frac{\scalar{#2}{#1}}{\scalar{#1}{#1}}}

\author{Anna Szczepaniak}
\title{Algorytm dekompozycji QR}
\begin{document}

\maketitle


\tableofcontents

%\chapter{Wstęp}

\chapter{Preliminaria}

W niniejszym rozdziale przypomniane zostaną wybrane elementy z zakresu algebry liniowej, potrzebne do wyjaśnienia działania algorytmu QR.

%\section{Elementy algebry liniowej} 

Niniejszą część rozpocznijmy od zdefiniowania pojęcia macierzy. Warto nadmienić, że w znaczącej części literatury - pojęcie te nie jest definiowane w sposób matematycznie precyzyjny.

\begin{definition}[Macierz \cite{poreda11}]
Niech $n,m \in \setN$. Macierzą o $m$ wierszach oraz $n$ kolumnach (nazywaną również macierzą o wymiarach $m \times n$) i wyrazach w ciele $\setR$ nazywamy funkcję 
$$
\mx{A}: \set{1,2, \ldots ,m}\times \set{1,2, \ldots ,n} \to \setR.
$$
Wartością funkcji dla argumentu $(i,j)$, gdzie $i \in \set{1,2, \ldots ,m}$, $j \in \set{1,2, \ldots ,n}$ jest element $a_{ij} \in \setR$. Macierz często zapisujemy w postaci tabeli
$$
\mx{A} = \begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1n} \\
         a_{21} & a_{22} & \cdots & a_{2n} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}.
$$

Przez $\setR^{m \times n}$ oznaczmy zbiór wszystkich macierzy o wymiarach $m \times n$ i elementach z $\setR$ .
\end{definition}

Macierze posiadają wiele istotnych dla nas podklas, posiadający określone cechy. Wymieńmy kilka typów macierzy, szczególnie interesujących z perspektywy omawianego przez nas tematu.

\begin{definition}[Macierz kwadratowa\citep{poreda11}] \label{definicja-macierzy}
Macierzą kwadratową o wymiarze $n$ na $n$ nazywamy macierz o równej liczbie wierszy i kolumn. Liczbę $n$ nazywamy wtedy stopniem macierzy kwadratowej.
\end{definition}

\begin{definition}[Macierz diagonalna\citep{poreda11}]
Jeśli w macierzy kwadratowej $[a_{ij}]$ wszystkie elementy poza główną przekątną są równe zeru, to taką macierz nazywamy macierzą diagonalną, oznaczamy ją jako $diag(a_{11}, a_{22},\ldots,a_{nn})$.
\end{definition}

\begin{definition}[Macierz jednostkowa \citep{poreda11}]
Macierz jednostkową stopnia $n$ nazywamy taką macierz diagonalną, w której wszystkie elementy na głównej przekątnej są równe 1.
\end{definition}

\begin{definition}[Macierz symetryczna]
Macierzą symetryczną nazywamy macierz kwadratową $\mx{A}=[a_{ij}]$, której wyrazy spełniają warunek 
$$
\forall_{i,j \in \set{1,\ldots,n}} \quad a_{ij}=a_{ji}.
$$
\end{definition}

\begin{definition}[Macierz trójkątna górna oraz trójkątna dolna\citep{poreda11}]
Jeśli w macierzy kwadratowej $[a_{ij}]$ wszystkie elementy poniżej głównej przekątnej są równe $0$, to taką macierz nazywamy macierzą trójkątną górną.
Analogicznie jeśli w macierzy kwadratowej $[a_{ij}]$ wszystkie elementy powyżej głównej przekątnej są równe $0$ to taką macierz nazywamy trójkątną dolną.
\end{definition}

\begin{definition}
Niech $\mx{A}=[a_{ij}] \in \setR^{m \times n}$. Wtedy macierz $D \in \setR^{n \times m}$ spełniająca warunek taki, że
$$
d_{ij} = a_{ji},
$$
dla dowolnych $i \in \set{1,\ldots, n}$ oraz $j \in \set{1,\ldots,m}$ nazywamy macierzą transponowaną do $A$. Z reguły tę macierz $D$ oznaczamy symbolem $\transpose{A}$.
\end{definition}

%TODO proszę kontynuować poprawki definicji poniżej

\begin{definition}[Iloczyn macierzy\citep{poreda11}]
Niech $\mx{A}\in \mx{M}_{m\times n}(\set{K})$ i $\mx{B}\in \mx{M}_{k\times m}(\set{K})$. Jeśli
$$
\mx{A} = \begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1n} \\
         a_{21} & a_{22} & \cdots & a_{2n} \\
         \vdots & \vdots & \ddots & \vdots \\
         a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}.
$$ 
i 
$$
\mx{B} = \begin{bmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
         b_{21} & b_{22} & \cdots & b_{2m} \\
         \vdots & \vdots & \ddots & \vdots \\
         b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{bmatrix}.
$$
to iloczynem macierzy $\mx{B}$ i $\mx{A}$ nazywamy macierz $\mx{C}$ taką, że
$$
\mx{C} = [c_{lj}]_{j=1,\ldots,n}^{l=1,\ldots,k}
$$
i
$$
c_{lj}= \sum_{l=1}^{m} b_{li} \cdot a_{ij}
$$.
\end{definition}

%[a_{ij}]_{i\le m, j\le k}$ będzie macierzą o wymiarach $m\times k$, a $\mx{B}=[b_{ij}]_{i\le k, j\le n}$ macierzą o wymiarach $k\times n$. Iloczynem macierzy $\mx{A}$,$\mx{B}$ nazywamy macierz $\mx{A} \mx{B}=[c_{ij}]_{i\le m, j\le n}$ o wymiarach $m\times n$, gdzie dla 
$$
i=1,\ldots,m, j=1,\ldots,n 
$$.


\begin{definition}[Macierz odwrotna\citep{poreda11}]
Macierz $\mx{A}$ ze zbioru $\mx{M}_{n}(\set{K})$ nazywamy macierzą odwracalną, jeśli istnieje macierz $\mx{B}$ w zbiorze.
Niech A będzie macierzą kwadratową ustalonego stopnia. Macierz A jest odwracalna, jeśli istnieje taka macierz B, że zachodzi
$$
A\cdot B=B\cdot A=I
$$, 
gdzie I jest macierzą jednostkową. Macierz B nazywa się wówczas macierzą odwrotną do macierzy A i oznacza się przez  $A^{-1}.$
\end{definition}

\begin{definition}
Macierz A nazywamy macierzą nieosobliwą, jeśli istnieje macierz B, która jest do niej odwrotna.
\end{definition}  

\begin{definition}
Macierzą ortogonalną nazywamy macierz kwadratową $A\in M_{n}(R)$ o elementach będących liczbami rzeczywistymi spełniająca równość:
$$
A^{T}\cdot A=A\cdot A^{T}=I_{n}
$$, 
gdzie $I_{n}$ oznacza macierz jednostkową wymiaru n, $A^{T}$ oznacza macierz transponowaną względem A.
\end{definition}

\begin{definition}
Dwie macierze kwadratowe A i B nazywamy macierzami podobnymi, jeśli istnieje taka macierz nieosobliwa P, że zachodzi związek: 
$$
B=P^{-1}\cdot A\cdot P
$$.
\end{definition}


\section{Definicje dotyczące wektorów} 

\begin{definition}
Jeżeli $\vr{p},\vr{q}\in \set{R}^{n}$, to parę $(\vr{p},\vr{q})$ nazywamy wektorem zaczepionym o początku $\vr{p}$ i końcu $\vr{q}$.
\end{definition}

\begin{definition}
Niech $\vr{v_{1}},..., \vr{v_{n}}$ będą różnymi elementami przestrzeni liniowe V. Zbiór wektorów ${\vr{v_{1}},...,\vr{v_{n}}}$ nazywamy liniowo niezależnymi jeżeli 
$$
\forall_{a_{1},...,a_{n}\in \mathbb{R}} (a_{1}\cdot v_{1} + ... + a_{n}\cdot v_{n} = 0 \implies a_{1}=...=a_{n}=0)
$$ 
\end{definition}

\section{Definicje dotyczące wyznacznika macierzy, jej wartości własnej oraz wektorów własnych.}

\begin{definition}[??]
Niech będzie dana macierz kwadratowa A stopnia n. Wyznacznikiem nazywamy takie odwzorowanie, które danej macierzy $A$ wymiaru $n \times n$ przyporządkowuje dokładnie jedną liczbę rzeczywistą $\det A$. Jeśli macierz jest stopnia $n = 1$, to jej wyznacznik $\det A  = a_{11}$. 
Jeśli stopień macierzy jest większy niż 1, to jej wyznacznik obliczamy według następującego wzoru: 
$$
\det A = \sum_{i=1}^{n} (-1)^{i+j}\cdot a_{ij}\cdot \det M_{ij},
$$
gdzie $\det M_{ij}$ oznacza wyznacznik macierzy powstałej z macierzy A przez skreślenie i-tego wiersza i j-tej kolumny.
\end{definition}

\begin{definition}
Niech $\mx{B}$ będzie macierzą wymiaru $m$ na $m$ i niech $I$ będzie macierzą jednostkową wymiaru $m$ na $m$. Wówczas wartościami własnymi macierzy $B$ nazywamy takie $\lambda \in \setC$, które spełniają następujące równanie:
$$
\det (\mx{B}-\lambda\cdot \mx{I})=0. 
$$ 
Ponadto każdy wektor $\vr{v} \in \setR^m$ spełniają dwa warunki
\begin{enumerate}
\item Nie będący wektorem zerowym - $v \neq 0$, oraz
\item spełniającym równość
$$
\mx{B} \vr{v} = \lambda \vr{v},
$$
\end{enumerate}
nazywać będziemy wektorem własnym stowarzyszonym z wartością własną $\lambda$.
\end{definition}

\chapter{Algorytm QR}

W tym rozdziale zaprezentujemy elementy teorii dokonywania rozkładów QR macierzy. Rozpoczniemy od sformułowania i udowodnienia twierdzenia o istnieniu takiego rozkładu. Wiele analizowanych pozycji literaturowych sygnalizowało posiadanie dowodu poniższego twierdzenia. W naszej ocenie prezentowane tam dowody bliższe są jednak jedynie ich szkicowi. Dla powyższego powoduje prezentujemy własne opracowanie dowodu twierdzenia o istnieniu rozkładu QR, stworzonego na podstawie szkiców omówionych w literaturze oraz pracy własnej. 

\begin{theorem}[O rozkładzie QR]\label{theorem-qr-docomposition}
Niech $\mx{A} \in \setR^{m \times n}$, gdzie $m\ge n$, której kolumny są liniowo niezależne. Istnieje wtedy jedyny rozkład 
$$
\mx{A} = \mx{Q} \mx{R}
$$ 
na dwa czynniki
\begin{itemize}
\item macierz $\mx{Q} \in \setR^{m \times n} $ taką, że 
$$
Q^{T}\cdot Q=D,
$$
gdzie $D= diag (d_{1}, d_{2}, ..., d_{n})$, oraz $d_{k}>0$ dla $k = 1, 2, \ldots, n$, oraz
\item macierz trójkątną górną $\mx{R} \in \setR^{n \times n}$ spełniającą dodatkowo warunek 
$$
r_{kk}= 1 
$$ 
dla $k = 1, 2, \ldots, n$.
\end{itemize} 
\end{theorem}

\begin{theorem}[Twierdzenie(Grama-Schmidta)\citep{poreda11}]
Dla każdego układu liniowo niezależnego wektorów $\vr(x_{1}),\ldots,\vr(x_{n})$ w przestrzeni euklidesowej istnieje układ ortonormalny $(\vr(b_{1},\ldots,b_{n})$ taki, że 
$$
span(\vr(b_{1},\ldots,b_{k}) = span(\vr(x_{1},\ldots,x_{k})
$$
dla każdej liczby $k$ ze zbioru $(1,\ldots,n)$.
\end{theorem}

\begin{proof}[Dowód\citep{poreda11}]
Oczywiście wektory $\vr(x_{i})$ są niezerowe, zatem mają długość dodatnią. Niech więc
$$
\vr(b_{1})=(1\div\Vert\vr(x_{1})\Vert)\cdot\vr(x_{1})
$$

\end{proof}
TODO twierdzenie Grama Schmidta z poredy + dowód


\begin{lemma}[O postaci macierzowej w algorytmie Grama-Schmidta] \label{lemma-matrix-formulation-of-gs}
Powyżej omówiony algorytm Grama Schmidta jest równoważny następującym transformacjom macierzy. Załóżmy, że wektory $\vr{v_1}, \ldots, \vr{v_n}$ są kolumnami macierzy $\mx{A}$ oraz że są liniowo niezależne. Wtedy z twierdzenia \ref{theorem-gram-schmidt} istnieją wektory $\vr{u_1}, \ldots, \vr{u_n}$ , które są transformacją wektorów $\vr{v_1}, \ldots, \vr{v_n}$ przez algorytm Grama-Schmidta. Niech $\mx{U}$ będzie macierza utworzoną kolumnowo z tych wektorów $\vr{u_1}, \ldots, \vr{u_n}$. Wtedy
{\small
$$
U = A \cdot 
\begin{bmatrix}
1 & -\proj{u_1}{v_2} & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix} \cdot
\begin{bmatrix}
1 & 0 & -\proj{u_1}{v_3} &\cdots & 0 \\
0 & 1 & -\proj{u_2}{v_3} &\cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots &  & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
 \cdots
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 & -\proj{u_1}{v_n} \\
0 & 1 & 0 & \cdots & 0 &-\proj{u_2}{v_n} \\
0 & 0 & 1 & \cdots & 0 & -\proj{u_3}{v_n} \\
\vdots & \vdots & \vdots &  & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & -\proj{u_{n-1}}{v_n} \\
0 & 0 & 0 & \cdots & 0 & 1
\end{bmatrix}
$$ 
}
\end{lemma}
\begin{proof}
Dówód powyższego faktu jest niemal automatyczny. Wykonujemy operacje kolejnych iteracji algorytmu Grama-Schmidta. Pierwsza z operacji odpowiada utworzeniu wektora $u_2$ i ławo zauważyć, że powyższe mnożenie macierzowe dostarcza dokładnie tę operację. Macierz jest prawie jednostkową, jedynie 2 kolumna się wyróżnia. W efekcie oznacza to, że wszystkie zmiany przez nią wykonane zostaną składowane w drugiej kolumnie macierzy wynikowej. Dokładnie jak w algorytmie. Kolejne iteracje algorytmu modyfikują wyniki działania poprzednich iteracji - stąd należy w mnożeniu macierzowym zastosować mnożenie wyniku poprzedniej iteracji z prawej strony. 
\end{proof}

\begin{lemma}[O iloczynie macierzy trójkątnych górnych] \label{lemma-upper-triangle-multiplication}
Niech $\mx{A}, \mx{B} \in \setR^{n \times n}$ bedą dwiema nieosobliwymi macierzami trójkątnymi górnymi. Wtedy
$$
\mx{C} = \mx{A} \cdot \mx{B},
$$
jest też nieosobliwą macierzą trójkątną górną.
\end{lemma}

\begin{proof}
Niech $\mx{A}, \mx{B}$ jak w założeniach, oraz $\mx{C} = \mx{A} \cdot \mx{B}$. 
Korzystająć z twierdzenia Cauchy'ego o iloczynie macierzy otrzymujemy natychmiast, że macierz $\mx{C} \in \setR^{n \times n}$ jest również nieosobliwa. Pozostaje pokazać, że jest macierzą trójkątną górną. Rozważmy dowolny $i,j \in \set{1, \ldots, n}$ parę indeksów z poniżej przekątnej, tj. takich dla których $i > j$. Wtedy
\begin{equation}\label{eq-proof-upper-triangle-mutliplication}
c_{ij} = a_{i1} b_{1j} + \ldots +a_{in} b_{nj}.
\end{equation}
Zauważmy, że elementy $a_{i1}, \ldots , a_{i(i-1)}$ są równe $0$ gdyż pochodzą z macierzy trójkątnej górnej. Ponadto elementy $b_{j(j+1)}, \ldots, b_{jn}$ są z analogicznego powodu również równe $0$. Skoro $i > j$ to każdy składnik sumy \eqref{eq-proof-upper-triangle-mutliplication} posiada czynnik równy $0$. Zatem $c_{ij}$ dla rozważanego indeksu jest równe $0$. W dowolności wyboru $i,j$ macierz $\mx{C}$ jest macierzą trójkątną górną.
\end{proof}

\begin{lemma}[Macierz transformująca algorytmu Grama-Schmidta]\label{lemma-gram-schmidt-matrix}
Niech $\mx{A} \in \setR^{m \times n}$ będzie macierzą, której kolumny są liniowo niezależne. Niech $\mx{U} \in \setR^{m \times n}$ będzie macierzą uzyskaną poprzez połączenie jako kolumn wektorów uzyskanych z algorytmu Grama-Schmidta. Wtedy istnieje trójkąta górna macierz $\mx{T} \in \setR^{n \times n}$ taka, że
$$
\mx{U} = \mx{A} \cdot \mx{T},
$$
i $T_{kk} = 1$ dla dowolnego $k \in \set{1,\ldots,n}$.
\end{lemma}

\begin{proof}
Niech $\mx{A}, \mx{U}$ takie jak w założeniach lematu. Wtedy wobec lematu \ref{lemma-matrix-formulation-of-gs} zachodzi 
$$
U = A \cdot 
\begin{bmatrix}
1 & -\proj{u_1}{v_2} & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix} \cdot
\begin{bmatrix}
1 & 0 & -\proj{u_1}{v_3} &\cdots & 0 \\
0 & 1 & -\proj{u_2}{v_3} &\cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots &  & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
 \cdots
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 & -\proj{u_1}{v_n} \\
0 & 1 & 0 & \cdots & 0 &-\proj{u_2}{v_n} \\
0 & 0 & 1 & \cdots & 0 & -\proj{u_3}{v_n} \\
\vdots & \vdots & \vdots &  & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & -\proj{u_{n-1}}{v_n} \\
0 & 0 & 0 & \cdots & 0 & 1
\end{bmatrix}.
$$ 
Zauważmy, że mnożenie macierzowe jest łączne. Możemy je mnożyć wykonując kolejne mnożenia patrząc od prawej strony. Stosując $n-2$ krotnie lemat \ref{lemma-upper-triangle-multiplication} otrzymujemy macierz $\mx{T}$ która jest nieosobliwą macierzą trójkątną górną. Łatwo zauważyć, że na przekątnej tej macierzy wszystkie elementy to właśnie $1$. 
\end{proof}

Istotną obserwacją jaką można poczynić jest taka, że macierzy tej nie można sobie w łatwy sposób od tak wyznaczyć. Aby ją uzyskać trzeba przeprowadzić cały algorytm Grama-Schmidta i dopiero po jego przeprowadzeniu macierz tę daje się jawnie wyznaczyć.

\begin{lemma}[O odwracaniu macierzy trójkątnej górnej]\label{lemma-upper-triangle-invertion}
Niech $\mx{A} \in \setR^{n \times n}$ będzie nieosobliwą macierzą trójkątną górną. Wtedy istnieje macierz $\inverse{\mx{A}}$ i jest ona nieosobliwą macierzą trójkątną górną.
\end{lemma}

\begin{proof}
Niech $\mx{A}$ będzie macierzą jak w założeniach twierdzenia.
Na mocy wniosku z twierdzenia Cauchy'ego o iloczynie macierzy wiemy, że macierz $\inverse{\mx{A}} $ istnieje i jest nieosobliwa. Pozostaje pokazać, że jest macierzą trójkątną. Pokazać ten fakt można na kilka różnych sposobów, zaprezentujemy tutaj najprostszy pod względem wiedzy teoretycznej dowód.
Wykorzystamy drugi krok z algorytmu elimininacji Gaussa. Przypomnijmy, że algorytm ten służy rozwiązaniu układu równań postaci 
$$
\mx{A} \vr{x} = \vr{b},
$$
i składa się z dwóch kroków. Pierwszym jest redukcja macierzy metodą eliminacji do macierzy trójkątnej górnej. Drugim etapem jest iteracyjne policzenie rozwiązania. Przyjmijmy, że mamy
$$
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
0 & a_{22} & \ldots & a_{2n} \\
\vdots & \vdots & & \vdots\\
0 & 0 & \ldots & a_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}.
$$
Skoro macierz $\mx{A}$ jest nieosobliwa to wiemy, że elementy na przekątnej są różne od $0$, tzn.
$$
\forall k \in \set{1, \ldots, n} \quad a_{kk} \neq 0.
$$
Wtedy prawdziwy jest wzór pozwalający na iteracyjne policzenie 
\begin{equation}
x_k = \frac{1}{a_{kk}} \bracket{b_k - \sum_{i=k+1}^{n} a_{ki} x_{i}}.
\end{equation}
{\color{red} Poniższa część dowodu, jest trochę dziurawa - może należałoby ją jeszcze rozszerzyć. Ogólnie chodzi o to, że $x_k$ zależy jedynie od $b_k, \ldots, b_n$ co sprawia, że ta transformacja odwrotna jest reprezentowana przez macierz trójkątną górną.}
W powyższej formule zauważmy, że istnieje takie $\alpha_n \in \setR^{n}$, że $x_n = \alpha_n \cdot
\transpose{\begin{bmatrix}
0, 0 , \ldots, 0, b_n
\end{bmatrix}}$. W efekcie istnieje takie $\alpha_{n-1} \in \setR^{n}$, że $x_{n-1}= \alpha_{n-1} \cdot
\transpose{\begin{bmatrix}
0, 0 , \ldots, b_{n-1}, b_n
\end{bmatrix}} $, a ostatecznie
\begin{equation} \label{eq-proof-lemma-upper-triangle-invertion}
\exists_{\alpha_1,\ldots,\alpha_n \in \setR^{n}}
\forall_{b \in \setR^{n} } 
\forall_{k \in \set{1,\ldots, n}} \quad x_{k} = \alpha_{k} \cdot
\transpose{\begin{bmatrix}
0, \ldots, 0 , b_{k} , \ldots, b_n
\end{bmatrix}}. 
\end{equation}
Przypuśćmy, że macierz $\inverse{\mx{A}}$ nie jest trójkątna górna. Wtedy istnieje pewien element ${\inverse{a}}_{ij} \neq 0$, gdzie $i > j$. To z kolei oznaczałoby, że \eqref{eq-proof-lemma-upper-triangle-invertion} nie jest spełnione. Zatem macierz ta istotnie musi być trójkątną górną.
\end{proof}

Teraz możemy przystąpić do dowodu głównego twierdzenia.

\begin{proof}[Dowód twierdzenia \ref{theorem-qr-docomposition}]
Niech $\mx{A} \in \setR^{m \times n}$ będzie macierzą jak w założeniach. Wtedy z lematu \ref{lemma-gram-schmidt-matrix} przy zastosowaniu algorytmu Grama-Schmidta otrzymujemy równość
$$
\mx{U} = \mx{A} \cdot \mx{T}.
$$
Na mocy lematu \ref{lemma-upper-triangle-invertion} wiemy, że macierz $\mx{T}$ jest odwracalna i macierz do niej odwrotna jest trójkątną górną. Wykonując mnożenie prawostronne przez tę macierz otrzymujemy:
\begin{align*}
\mx{U} \cdot \inverse{\mx{T}} & = \mx{A}\cdot\mx{T}\cdot \inverse{\mx{T}}, \\
\mx{U} \cdot \inverse{\mx{T}} & = \mx{A}.
\end{align*}  
Postać którą mamy powyżej jest już bliska faktycznemu rozkładowi QR. Macierze te jednak różnią się własnościami. Macierz $\mx{U}$ nie jest macierzą ortogonalną. Jest utworzona z kolumn które są wzajemnie ortogonalne, ale macierz ortogonalna musi posiadać znomalizowane ortonormalne kolumny. Trzeba wprowadzić w nich zatem pewną korektę. Rozważmy macierz $\mx{D}$ następującej postaci
$$
\mx{D} = \begin{bmatrix}
\sqrt{\scalar{u_1}{u_1}} & 0 & 0 & \ldots & 0 \\
0 & \sqrt{\scalar{u_2}{u_2}} & 0 & \ldots & 0 \\
0 & 0 & \sqrt{\scalar{u_3}{u_3}} & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \ldots & \sqrt{\scalar{u_n}{u_n}}
\end{bmatrix}.
$$
Zauważmy natychmiast, że 
$$
\inverse{\mx{D}} = 
\begin{bmatrix}
\frac{1}{\sqrt{\scalar{u_1}{u_1}}} & 0 & 0 & \ldots & 0 \\
0 & \frac{1}{\sqrt{\scalar{u_2}{u_2}}} & 0 & \ldots & 0 \\
0 & 0 & \frac{1}{\sqrt{\scalar{u_3}{u_3}}} & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \ldots & \frac{1}{\sqrt{\scalar{u_n}{u_n}}}
\end{bmatrix}.
$$
Wtedy możemy wykonać następujące mnożenia
\begin{align*}
\mx{U} \cdot \mx{I_n} \cdot \inverse{\mx{T}} & = \mx{A} ,\\
\mx{U} \cdot \inverse{\mx{D}} \cdot \mx{D} \cdot \inverse{\mx{T}} & = \mx{A}.
\end{align*}
Wtedy oznaczmy $\mx{Q}= \mx{U} \cdot \inverse{\mx{D}}$ oraz $ \mx{R} = \mx{D}\cdot \inverse{\mx{T}}$. Zauważmy, że tak otrzymana macierz $\mx{Q}$ jest ortonormalna. Natomiast $\mx{R}$ jako iloczyn dwóch macierzy górnie trójkątnych jest macierzą górnie trójkątną. Dokładne rachunki pokazały by ponadto, że elementy macierzy $\inverse{\mx{T}} $ na przekątnej były $1$ co sprawia, że na przekątnej znajdują się dokładnie elementy macierzy $\mx{D}$.

\end{proof}

\noindent \textbf{Dowód twierdzenia 3.1}\\
Podane wyżej twierdzenie jest przekształceniem
procesu ortogonalizacji Grama-Schmidta. Jeśli zastosujemy Grama-Schmidta do kolumn $a_{i}$ macierzy
 $A = [a_{1}, a_{2}, ..., a_{n}]$ od lewej do prawej, otrzymamy
sekwencje ortonormalnych wektorów od $q_{1}$ do $q_{n}$ obejmujących tę samą przestrzeń:
te ortogonalne wektory są kolumnami Q. Gram-Schmidt również wylicza
współczynniki wyrażające każdą kolumnę $a_{i}$
 jako liniową kombinację $q_{1}$ przez 
 $$q_{i}: a_{i}= \sum_{j=1}^i r_{ji}\cdot q_{i}, $$ 
 gdzie $r_{ji}$ to współczynniki macierzy R.
 



Algorytm QR został wynaleziony w 1961 roku przez Francisa i Kubłanowską. Jest jedną z efektywniejszych znanych metod rozwiązywania pełnego zadania własnego dla macierzy symetrycznych lub niesymetrycznych.
W podstawowym algorytmie QR tworzy się ciąg macierzy $A= A_{0}, A_{1}, A_{2}, ...$ taki, że 
$$ A_{s}=Q_{s}R_{s},$$
$$R_{s}Q_{s}=A_{s+1},$$
$$(s=0, 1, ...),$$ 
gdzie $Q_{s}$ jest macierzą ortogonalną, a $R_{s}$ - trójkątną górną. Łatwo widać, że z twierdzenia o rozkładzie QR wynika, że ciąg $A_{s}$ $(s=0, 1, ...)$ jest w zasadzie określony jednoznacznie. Ponieważ 
$$ A_{s+1}=R_{s}Q_{s}=Q_{s}^{T}A_{s}Q_{s},$$ 
więc każdy krok w algorytmie QR jest przekształceniem przez podobieństwo. 

\subsection*{Metoda Householdera}

Metoda Householdera pozwala znaleźć rozkład QR dowolnej macierzy prostokątnej m x n ($m\ge n$).

{Macierz Householdera}
Macierzą Householdera H zwaną również refleksją nazywamy symetryczną i ortogonalną macierz przekształcenia wektora, które odbija go względem pewnej płaszczyzny. 
\newpage
{Transformacja Householdera}
Niech  $v\in R^{m}, $ i $v\neq 0. $ Wówczas transformacją Householdera nazywamy macierz postaci:

$${\displaystyle H=I-Wvv^{T}, } {\displaystyle H=I-Wvv^{T}, }       {\displaystyle W={\frac {2}{v^{T}v}}} {\displaystyle W={\frac {2}{v^{T}v}}}$$ 
Macierz H jest macierzą symetryczną i ortogonalną oraz ma taką własność, że dowolny wektor x wymiaru m jest odbiciem lustrzanym wektora Hx względem hiperpłaszczyzny (wymiaru m-1) prostopadłej do wektora v[3]. Łatwo sprawdzić, że tak jest ponieważ: 

$${\displaystyle H^{2}=\left(I-{\frac {2vv^{T}}{v^{T}v}}\right)^{2}=I-{\frac {4vv^{T}}{v^{T}v}}+4\left({\frac {vv^{T}}{v^{T}v}}\right)^{2}=I} {\displaystyle H^{2}=\left(I-{\frac {2vv^{T}}{v^{T}v}}\right)^{2}}$$ \newline  $$= {I-{\frac {4vv^{T}}{v^{T}v}}+4\left({\frac {vv^{T}}{v^{T}v}}\right)^{2}=I}       {\displaystyle ((vv^{T})(v^{T}v)=(vv^{T})^{2})} {\displaystyle ((vv^{T})(v^{T}v)=(vv^{T})^{2})}$$\\
oraz\\

$${\displaystyle H^{T}=\left(I-{\frac {2vv^{T}}{v^{T}v}}\right)^{T}=I-\left({\frac {2vv^{T}}{v^{T}v}}\right)^{T}=I-{\frac {2vv^{T}}{v^{T}v}}=H} {\displaystyle H^{T}=\left(I-{\frac {2vv^{T}}{v^{T}v}}\right)^{T}}$$ \newline $$={I-\left({\frac {2vv^{T}}{v^{T}v}}\right)^{T}=I-{\frac {2vv^{T}}{v^{T}v}}=H}       {\displaystyle ((vv^{T})^{T}=(vv^{T}))} {\displaystyle ((vv^{T})^{T}=(vv^{T}))}$$\\
Z drugiej równości wynika symetria, z pierwszej ortogonalność, ponieważ $${\displaystyle H^{T}H=HH=I}. $$ Zatem:\\

$${\displaystyle |Hx|={\sqrt {(Hx)^{T}(Hx)}}={\sqrt {x^{T}(H^{T}H)x}}={\sqrt {x^{T}Ix}}=|x|} {\displaystyle |Hx|={\sqrt {(Hx)^{T}(Hx)}}={\sqrt {x^{T}(H^{T}H)x}}={\sqrt {x^{T}Ix}}=|x|}. $$\\
Mnożąc dowolny wektor ${\displaystyle x\in R^{m}}$ otrzymujemy:\\

$${\displaystyle Hx=x-{\frac {2vv^{T}x}{v^{T}v}}=x+(-2{\frac {vv^{T}x}{v^{T}v}})=x-2r} {\displaystyle Hx=x-{\frac {2vv^{T}x}{v^{T}v}}=x+(-2{\frac {vv^{T}x}{v^{T}v}})=x-2r}$$





\chapter{Eksperymenty numeryczne}

\chapter{Podsumowanie}


\bibliographystyle{plain}
\bibliography{bibliografia}

\end{document}